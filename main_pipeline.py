import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.ensemble import HistGradientBoostingRegressor
import matplotlib.pyplot as plt

# Import hÃ m xá»­ lÃ½ log tá»« file cá»§a Ä‘á»“ng Ä‘á»™i
try:
    from auto_scaling import load_log, aggregate_per_minute, add_status_features
except ImportError:
    print("âŒ Lá»—i: Cáº§n file 'auto_scaling.py' á»Ÿ cÃ¹ng thÆ° má»¥c Ä‘á»ƒ parse log.")
    exit()

# ==========================================
# 1. DATA PROCESSING (Gá»™p 2 model)
# ==========================================
def process_log_to_5m(log_path):
    """Äá»c log -> 1 phÃºt -> 5 phÃºt (Request + Bytes + Status)"""
    if not Path(log_path).exists():
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y: {log_path}")
        return None
        
    df_raw = load_log(log_path)
    df_1m = aggregate_per_minute(df_raw)
    df_1m = add_status_features(df_raw, df_1m)
    
    # Gom 5 phÃºt: Sum háº¿t cÃ¡c chá»‰ sá»‘
    agg_rules = {
        'requests': 'sum', 'bytes': 'sum',
        'status_200': 'sum', 'status_500': 'sum'
    }
    # Tá»± Ä‘á»™ng thÃªm cÃ¡c cá»™t status khÃ¡c náº¿u cÃ³
    for c in df_1m.columns:
        if 'status_' in c and c not in agg_rules:
            agg_rules[c] = 'sum'
            
    df_5m = df_1m.resample('5min').agg(agg_rules).fillna(0)
    return df_5m

def feature_engineering(df):
    df = df.copy()
    
    # --- Features cho REQUEST ---
    df['hour'] = df.index.hour
    df['dayofweek'] = df.index.dayofweek
    df['ratio_5xx'] = df['status_500'] / (df['requests'] + 1e-9)
    df['is_crash'] = (df['ratio_5xx'] > 0.2).astype(int)
    
    # Lag Requests
    df['lag_requests_1'] = df['requests'].shift(1)
    df['lag_requests_288'] = df['requests'].shift(288)

    # --- Features cho BYTES ---
    # Lag Bytes
    df['lag_bytes_1'] = df['bytes'].shift(1)
    df['lag_bytes_288'] = df['bytes'].shift(288)
    
    return df

# ==========================================
# 2. MAIN PIPELINE
# ==========================================
def main():
    # PATH
    raw_dir = Path("F:/projects/DataflowSS2/")
    
    # A. Xá»¬ LÃ Dá»® LIá»†U
    print("ğŸš€ Äang xá»­ lÃ½ log Train & Test...")
    train_df = process_log_to_5m(raw_dir / "train.txt")
    test_df = process_log_to_5m(raw_dir / "test.txt")
    
    if train_df is None or test_df is None: return

    # Ná»‘i láº¡i Ä‘á»ƒ táº¡o lag (trÃ¡nh Ä‘á»©t gÃ£y giá»¯a thÃ¡ng 7 vÃ  8)
    full = pd.concat([train_df, test_df]).sort_index()
    full = feature_engineering(full)
    
    # TÃ¡ch Train/Test (Má»‘c 22/08)
    split_date = "1995-08-22 23:59:59-04:00"
    train_final = full[full.index <= split_date].dropna()
    test_final = full[full.index > split_date].copy()
    test_final = test_final.fillna(method='bfill') # Fix lag Ä‘áº§u chuá»—i

    # B. TRAIN MODEL 1: REQUESTS (requests)
    print("ğŸ§  Training Model 1: Requests...")
    feats_req = ['hour', 'dayofweek', 'lag_requests_1', 'lag_requests_288', 'ratio_5xx', 'is_crash']
    model_req = HistGradientBoostingRegressor(random_state=42)
    model_req.fit(train_final[feats_req], np.log1p(train_final['requests']))
    
    # Dá»± bÃ¡o Request
    pred_requests_log = model_req.predict(test_final[feats_req])
    test_final['pred_requests'] = np.expm1(pred_requests_log)

    # C. TRAIN MODEL 2: BYTES (BÄƒng thÃ´ng)
    print("ğŸ§  Training Model 2: Bytes...")
    # Máº¹o: DÃ¹ng 'requests' thá»±c táº¿ Ä‘á»ƒ train, nhÆ°ng dÃ¹ng 'pred_requests' Ä‘á»ƒ predict
    # Äiá»u nÃ y giÃºp model Bytes hÆ°á»Ÿng lá»£i tá»« Ä‘á»™ chÃ­nh xÃ¡c cá»§a model Requests
    
    feats_bytes = ['hour', 'dayofweek', 'lag_bytes_1', 'lag_bytes_288', 'ratio_5xx']
    
    # ThÃªm feature 'requests' vÃ o training (requests thá»±c táº¿)
    X_train_bytes = train_final[feats_bytes].copy()
    X_train_bytes['current_requests'] = train_final['requests'] 
    
    model_bytes = HistGradientBoostingRegressor(random_state=42)
    model_bytes.fit(X_train_bytes, np.log1p(train_final['bytes']))
    
    # Dá»± bÃ¡o Bytes (DÃ¹ng 'pred_requests' lÃ m Ä‘áº§u vÃ o thay vÃ¬ requests thá»±c táº¿ - Ä‘á»ƒ trÃ¡nh leak)
    X_test_bytes = test_final[feats_bytes].copy()
    X_test_bytes['current_requests'] = test_final['pred_requests'] # Quan trá»ng!
    
    pred_bytes_log = model_bytes.predict(X_test_bytes)
    test_final['pred_bytes'] = np.expm1(pred_bytes_log)

    # D. XUáº¤T Káº¾T QUáº¢
    out_cols = ['requests', 'pred_requests', 'bytes', 'pred_bytes']
    test_final[out_cols].to_csv("submission_final.csv")
    print("ğŸ‰ Xong! File káº¿t quáº£: submission_final.csv")
    
    # Váº½ Ä‘á»“ thá»‹ kÃ©p
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)
    
    # Plot Request
    ax1.plot(test_final.index, test_final['requests'], color='black', alpha=0.3, label='Real requests')
    ax1.plot(test_final.index, test_final['pred_requests'], color='blue', label='Predicted requests')
    ax1.set_title("Dá»± bÃ¡o Requests (Model 1)")
    ax1.legend()
    
    # Plot Bytes
    ax2.plot(test_final.index, test_final['bytes'], color='gray', alpha=0.3, label='Real Bytes')
    ax2.plot(test_final.index, test_final['pred_bytes'], color='green', label='Predicted Bytes')
    ax2.set_title("Dá»± bÃ¡o Bytes (Model 2)")
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig("forecast_full.png")
    print("ğŸ“Š ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: forecast_full.png")

if __name__ == "__main__":
    main()